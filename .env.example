# ============================================================================
# LLM Provider Configuration
# ============================================================================
# Choose your LLM provider: ollama, openai, or claude
LLM_PROVIDER=ollama

# ============================================================================
# Security
# ============================================================================
# JWT secret for authentication tokens
# IMPORTANT: Set this in production! Generate with: python -c "import secrets; print(secrets.token_hex(32))"
# If not set, a random secret is generated (sessions lost on restart)
# JWT_SECRET=your-secure-secret-here

# Common LLM settings
LLM_TEMPERATURE=0.7

# --- Ollama Configuration ---
# Use this for local Ollama models
OLLAMA_MODEL=llama3.2:1b
OLLAMA_BASE_URL=http://localhost:11434

# --- OpenAI Configuration ---
# Use this for OpenAI models (GPT-4, GPT-3.5, etc.)
# OPENAI_API_KEY=your-api-key-here
# OPENAI_MODEL=gpt-4o

# --- Claude (Anthropic) Configuration ---
# Use this for Claude models
# ANTHROPIC_API_KEY=your-api-key-here
# ANTHROPIC_MODEL=claude-3-5-sonnet-20241022

# ============================================================================
# Observability
# ============================================================================
PHOENIX_PORT=6006
ENABLE_OBSERVABILITY=true

# Set to false to disable Phoenix observability (useful if port conflicts occur)
# The system will work fine without observability - it just won't have tracing

# ============================================================================
# Prometheus Metrics
# ============================================================================
# Enable Prometheus metrics endpoint at /metrics
# Use with: ./magentic.sh metrics (starts Prometheus + Grafana)
ENABLE_METRICS=true
# Grafana: http://localhost:3002 (admin/magentic123)
# Prometheus: http://localhost:9090

# ============================================================================
# Logging
# ============================================================================
LOG_LEVEL=INFO
LOG_FILE=agent.log

# Enable real-time state visualization (shows agent outputs and state at each step)
# Set to 'true' to see detailed state snapshots during execution
# Useful for debugging state flow issues
DEBUG_STATE=false

# ============================================================================
# System Limits
# ============================================================================
MAX_INPUT_LENGTH=1000
MAX_PARALLEL_AGENTS=3          

# Limit concurrent agent executions to prevent system overload
# Higher values = faster but more resource intensive
# Lower values = slower but more stable
# Recommended: 2-5 depending on your system

# UI Display character limit for agent inputs/outputs
# Controls how many characters are shown in the UI for each agent's input/output
# Full content is still preserved internally for agent-to-agent communication
# Minimum: 50, Default: 200
UI_DISPLAY_LIMIT=200

# Agent context limits (for inter-agent communication)
# Maximum characters passed from previous agent outputs to next layer
# Increase if you see truncated context between agents
AGENT_CONTEXT_LIMIT=4000

# Maximum characters for conversation history preview per step
AGENT_HISTORY_LIMIT=500

# Delegation depth limits (for hierarchical agent execution)
# Default max depth for agent delegation chains
# Higher = more delegation levels allowed, Lower = flatter execution
MAX_DELEGATION_DEPTH=3

# Hard safety limit to prevent infinite recursion (must be >= MAX_DELEGATION_DEPTH)
ABSOLUTE_MAX_DEPTH=5

# Maximum subtasks an agent can delegate to at once (prevents explosion)
MAX_SUBTASKS_PER_DELEGATION=5

# Maximum total delegations allowed per query (across all depths)
# This is the ultimate safety limit to prevent infinite delegation loops
MAX_TOTAL_DELEGATIONS=20

# ============================================================================
# RAG (Retrieval-Augmented Generation) Configuration
# ============================================================================
# Enable RAG for knowledge base retrieval
ENABLE_RAG=false

# Vector store backend: 'qdrant' (recommended, production-ready) or 'chromadb' (simple)
RAG_VECTOR_STORE=qdrant

# Qdrant Configuration
# Mode: 'memory' for embedded (no server needed) or 'server' for remote Qdrant server
RAG_QDRANT_MODE=memory
# Qdrant server URL (only needed for server mode)
RAG_QDRANT_URL=http://localhost:6333
# Qdrant collection name
RAG_QDRANT_COLLECTION=knowledge_base

# Directory to persist vector store (for embedded mode or ChromaDB)
RAG_PERSIST_DIRECTORY=./rag_data

# Text splitting configuration
RAG_CHUNK_SIZE=1000
RAG_CHUNK_OVERLAP=200

# Number of documents to retrieve
RAG_TOP_K=4

# Embedding provider: defaults to LLM_PROVIDER
# ollama -> local embeddings (free)
# openai -> OpenAI embeddings
# claude -> Voyage AI embeddings (Anthropic's recommendation)
# voyage -> Voyage AI embeddings (explicit)
# Override only if you want different provider for embeddings
# RAG_EMBEDDING_PROVIDER=voyage

# Embedding model (auto-selected based on provider if not specified)
# For Ollama: nomic-embed-text (default), mxbai-embed-large
# For OpenAI: text-embedding-3-small (default), text-embedding-3-large
# For Voyage AI: voyage-3.5 (default), voyage-3-large, voyage-code-3, voyage-finance-2, voyage-law-2

# ============================================================================
# MCP (Model Context Protocol) Configuration
# ============================================================================
# Enable MCP for extended agent capabilities (filesystem, fetch, memory)
# Requires Docker: ./magentic.sh mcp
ENABLE_MCP=false

# MCP Gateway URL
MCP_GATEWAY_URL=http://localhost:9000
