# ============================================================================
# LLM Provider Configuration
# ============================================================================
# Choose your LLM provider: ollama, openai, or claude
LLM_PROVIDER=ollama

# Common LLM settings
LLM_TEMPERATURE=0.7

# --- Ollama Configuration ---
# Use this for local Ollama models
OLLAMA_MODEL=llama3.2:1b
OLLAMA_BASE_URL=http://localhost:11434

# --- OpenAI Configuration ---
# Use this for OpenAI models (GPT-4, GPT-3.5, etc.)
# OPENAI_API_KEY=your-api-key-here
# OPENAI_MODEL=gpt-4o

# --- Claude (Anthropic) Configuration ---
# Use this for Claude models
# ANTHROPIC_API_KEY=your-api-key-here
# ANTHROPIC_MODEL=claude-3-5-sonnet-20241022

# ============================================================================
# Observability
# ============================================================================
PHOENIX_PORT=6006
ENABLE_OBSERVABILITY=true

# Set to false to disable Phoenix observability (useful if port conflicts occur)
# The system will work fine without observability - it just won't have tracing

# ============================================================================
# Logging
# ============================================================================
LOG_LEVEL=INFO
LOG_FILE=agent.log

# ============================================================================
# System Limits
# ============================================================================
MAX_INPUT_LENGTH=1000
MAX_PARALLEL_AGENTS=3          

# Limit concurrent agent executions to prevent system overload
# Higher values = faster but more resource intensive
# Lower values = slower but more stable
# Recommended: 2-5 depending on your system
